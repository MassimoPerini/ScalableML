{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-image in d:\\anaconda3\\envs\\tensorflow\\lib\\site-packages\n",
      "Requirement already satisfied: cloudpickle>=0.2.1 in d:\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from scikit-image)\n",
      "Requirement already satisfied: networkx>=1.8 in d:\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from scikit-image)\n",
      "Requirement already satisfied: pillow>=4.3.0 in d:\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from scikit-image)\n",
      "Requirement already satisfied: PyWavelets>=0.4.0 in d:\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from scikit-image)\n",
      "Requirement already satisfied: dask[array]>=0.9.0 in d:\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from scikit-image)\n",
      "Requirement already satisfied: six>=1.10.0 in d:\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from scikit-image)\n",
      "Requirement already satisfied: decorator>=4.3.0 in d:\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from networkx>=1.8->scikit-image)\n",
      "Requirement already satisfied: numpy>=1.9.1 in d:\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from PyWavelets>=0.4.0->scikit-image)\n",
      "Requirement already satisfied: toolz>=0.7.3; extra == \"array\" in d:\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from dask[array]>=0.9.0->scikit-image)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using pip version 9.0.1, however version 18.1 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting PIL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Could not find a version that satisfies the requirement PIL (from versions: )\n",
      "No matching distribution found for PIL\n",
      "You are using pip version 9.0.1, however version 18.1 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "# comment out this line Massimo it is for me only\n",
    "!pip install scikit-image\n",
    "!pip install PIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.applications.inception_resnet_v2 import InceptionResNetV2\n",
    "from keras.preprocessing import image\n",
    "from keras.engine import Layer\n",
    "from keras.applications.inception_resnet_v2 import preprocess_input\n",
    "from keras.layers import Conv2D, UpSampling2D, InputLayer, Conv2DTranspose, Input, Reshape, merge, concatenate\n",
    "from keras.layers import Activation, Dense, Dropout, Flatten\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import TensorBoard \n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers.core import RepeatVector, Permute\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "from skimage.color import rgb2lab, lab2rgb, rgb2gray, gray2rgb\n",
    "from skimage.transform import resize\n",
    "from skimage.io import imsave\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from IPython.display import display \n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load imgs\n",
    "X = []\n",
    "for filename in os.listdir('train/'):\n",
    "    X.append(img_to_array(load_img('train/'+filename)))\n",
    "X = np.array(X, dtype = float)\n",
    "X_train = 1.0 / 255 * X\n",
    "\n",
    "inception = keras.applications.nasnet.NASNetLarge(weights='imagenet', include_top=True)\n",
    "inception.graph = tf.get_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encoder\n",
    "encoder_input = Input(shape=(256, 256, 1,))\n",
    "encoder_output = Conv2D(64, (3,3), activation='relu', padding='same', strides=2)(encoder_input)\n",
    "encoder_output = Conv2D(128, (3,3), activation='relu', padding='same')(encoder_output)\n",
    "encoder_output = Conv2D(128, (3,3), activation='relu', padding='same', strides=2)(encoder_output)\n",
    "encoder_output = Conv2D(256, (3,3), activation='relu', padding='same')(encoder_output)\n",
    "encoder_output = Conv2D(256, (3,3), activation='relu', padding='same', strides=2)(encoder_output)\n",
    "encoder_output = Conv2D(512, (3,3), activation='relu', padding='same')(encoder_output)\n",
    "encoder_output = Conv2D(512, (3,3), activation='relu', padding='same')(encoder_output)\n",
    "encoder_output = Conv2D(256, (3,3), activation='relu', padding='same')(encoder_output)\n",
    "\n",
    "#Fusion\n",
    "embed_input = Input(shape=(1000,))\n",
    "fusion_output = RepeatVector(32 * 32)(embed_input) \n",
    "fusion_output = Reshape(([32, 32, 1000]))(fusion_output)\n",
    "fusion_output = concatenate([encoder_output, fusion_output], axis=3) \n",
    "fusion_output = Conv2D(256, (1, 1), activation='relu', padding='same')(fusion_output) \n",
    "\n",
    "#Decoder\n",
    "decoder_output = Conv2D(128, (3,3), activation='relu', padding='same')(fusion_output)\n",
    "decoder_output = UpSampling2D((2, 2))(decoder_output)\n",
    "decoder_output = Conv2D(64, (3,3), activation='relu', padding='same')(decoder_output)\n",
    "decoder_output = UpSampling2D((2, 2))(decoder_output)\n",
    "decoder_output = Conv2D(32, (3,3), activation='relu', padding='same')(decoder_output)\n",
    "decoder_output = Conv2D(16, (3,3), activation='relu', padding='same')(decoder_output)\n",
    "decoder_output = Conv2D(2, (3, 3), activation='tanh', padding='same')(decoder_output)\n",
    "decoder_output = UpSampling2D((2, 2))(decoder_output)\n",
    "\n",
    "model = Model(inputs=[encoder_input, embed_input], outputs=decoder_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\skimage\\transform\\_warps.py:110: UserWarning: Anti-aliasing will be enabled by default in skimage 0.15 to avoid aliasing artifacts when down-sampling images.\n",
      "  warn(\"Anti-aliasing will be enabled by default in skimage 0.15 to \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 698s 70s/step - loss: 0.4219\n",
      "Epoch 2/5\n",
      "10/10 [==============================] - 635s 64s/step - loss: 0.0085\n",
      "Epoch 3/5\n",
      "10/10 [==============================] - 635s 63s/step - loss: 0.0140\n",
      "Epoch 4/5\n",
      "10/10 [==============================] - 667s 67s/step - loss: 0.0098\n",
      "Epoch 5/5\n",
      "10/10 [==============================] - 985s 98s/step - loss: 0.0101\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1efb4adf9e8>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def inception_embeddings(grays):\n",
    "    grayresized = []\n",
    "    for i in grays:\n",
    "#         NAS uses 331*331\n",
    "        i = resize(i, (331, 331, 3), mode='constant')\n",
    "        grayresized.append(i)\n",
    "    grayresized = np.array(grayresized)\n",
    "    grayresized = preprocess_input(grayresized)\n",
    "    with inception.graph.as_default():\n",
    "        embed = inception.predict(grayresized)\n",
    "    return embed\n",
    "\n",
    "# augmentation\n",
    "aug = ImageDataGenerator(\n",
    "        shear_range=0.5,\n",
    "        zoom_range=0.5,\n",
    "        rotation_range=10,\n",
    "        horizontal_flip=True)\n",
    "\n",
    "\n",
    "def image_gen(batch_size):\n",
    "    for batch in aug.flow(X_train, batch_size=batch_size):\n",
    "        grays = gray2rgb(rgb2gray(batch))\n",
    "        embed = inception_embeddings(grays)\n",
    "        lab_batch = rgb2lab(batch)\n",
    "        X_batch = lab_batch[:,:,:,0]\n",
    "        X_batch = X_batch.reshape(X_batch.shape+(1,))\n",
    "        Y_batch = lab_batch[:,:,:,1:] / 128\n",
    "        yield ([X_batch, inception_embeddings(grays)], Y_batch)\n",
    "\n",
    "\n",
    "#Train model\n",
    "batch_size = 15\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "model.fit_generator(image_gen(batch_size), epochs=5, steps_per_epoch=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\skimage\\transform\\_warps.py:110: UserWarning: Anti-aliasing will be enabled by default in skimage 0.15 to avoid aliasing artifacts when down-sampling images.\n",
      "  warn(\"Anti-aliasing will be enabled by default in skimage 0.15 to \"\n",
      "D:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\skimage\\util\\dtype.py:141: UserWarning: Possible precision loss when converting from float64 to uint8\n",
      "  .format(dtypeobj_in, dtypeobj_out))\n"
     ]
    }
   ],
   "source": [
    "tobecolored = []\n",
    "for filename in os.listdir('test/'):\n",
    "    tobecolored.append(img_to_array(load_img('test/'+filename)))\n",
    "tobecolored = np.array(tobecolored, dtype=float)\n",
    "grayout = gray2rgb(rgb2gray(1 / 255 * tobecolored))\n",
    "tobecolored_embed = inception_embeddings(grayout)\n",
    "# 331*331*1...blah\n",
    "tobecolored = rgb2lab(1 / 255 * tobecolored)[:,:,:,0]\n",
    "tobecolored = tobecolored.reshape(tobecolored.shape+(1,))\n",
    "\n",
    "\n",
    "# Test\n",
    "output = model.predict([tobecolored, tobecolored_embed])\n",
    "output = output * 128\n",
    "\n",
    "# Output\n",
    "for i in range(len(output)):\n",
    "    done = np.zeros((256, 256, 3))\n",
    "    done[:,:,0] = tobecolored[i][:,:,0]\n",
    "    done[:,:,1:] = output[i]\n",
    "    imsave(\"result/img_\"+ str(i)+\".png\", lab2rgb(done))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
