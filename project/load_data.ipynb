{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/massimo/anaconda/envs/tensorflow/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Conv2D, UpSampling2D, InputLayer, Conv2DTranspose\n",
    "from keras.layers import Activation, Dense, Dropout, Flatten\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.models import Sequential\n",
    "#from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "from keras.preprocessing import image as k_img\n",
    "from skimage.color import rgb2lab, lab2rgb, rgb2gray, xyz2lab, gray2rgb\n",
    "from skimage.io import imsave\n",
    "from skimage.transform import resize\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.applications.inception_resnet_v2 import preprocess_input\n",
    "import tensorflow as tf\n",
    "from keras.layers import *\n",
    "from keras.models import Model\n",
    "from keras.applications.inception_resnet_v2 import InceptionResNetV2\n",
    "\n",
    "inception = InceptionResNetV2(weights=\"dataset/inception_resnet_v2_weights_tf_dim_ordering_tf_kernels.h5\")\n",
    "#inception.load_weights(weights='imagenet')\n",
    "#inception = InceptionResNetV2(weights=\"imagenet\")\n",
    "\n",
    "inception.graph = tf.get_default_graph()\n",
    "\n",
    "image_gen = ImageDataGenerator(shear_range=0.4,zoom_range=0.4,rotation_range=40,horizontal_flip=True)\n",
    "#image_gen.fit(dataset)\n",
    "\n",
    "def generate_classification(batch_data):\n",
    "    resized = np.zeros((len(batch_data),299,299,3))\n",
    "    #fig2=plt.figure()\n",
    "    #plt.imshow(batch_data[0])\n",
    "    #fig2.show()\n",
    "    k=0\n",
    "    for i in batch_data:\n",
    "        resized[k] = resize(i, (299, 299, 3), mode='constant')#skimage resize, ResNet V2 models use input image size of 299\n",
    "        k+=1\n",
    "    resized = preprocess_input(resized*255)\n",
    "    with inception.graph.as_default():\n",
    "        embed = inception.predict(resized)\n",
    "    return embed\n",
    "def data_generator(batch_size = 20):\n",
    "    \n",
    "    for batch in image_gen.flow(dataset, batch_size=batch_size):\n",
    "    \n",
    "        classification = generate_classification(gray2rgb(rgb2gray(batch)))# 3-layers grey image, InceptionResNetV2 works with RGB but we can't use informations obtained from a full RGB picture\n",
    "        lab = rgb2lab(batch)\n",
    "        x = lab[:,:,:,0]#grey scale of the all batch (batch, 256,256)\n",
    "        x = x.reshape(x.shape+(1,)) #(batch, 256, 256, 1) 1 layer, 256*256, batch pics\n",
    "        y = lab[:,:,:,1:] / 128 #(batch, 256, 256, 2)\n",
    "        yield ([x, classification], y)#[x,classification] input of network\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_gen_args = dict(#featurewise_center=True,\n",
    "                     #featurewise_std_normalization=True,\n",
    "                     rotation_range=90,\n",
    "                     width_shift_range=0.1,\n",
    "                     height_shift_range=0.1,\n",
    "                     zoom_range=0.2)\n",
    "image_datagen = ImageDataGenerator(**data_gen_args)\n",
    "\n",
    "\n",
    "def input_generator(image_datagen, batch_size = 20):\n",
    "    for batch in image_datagen.flow_from_directory('dataset/images/Train/',class_mode=None,batch_size = batch_size):\n",
    "        \n",
    "        classification = generate_classification(gray2rgb(rgb2gray(batch)))# 3-layers grey image, InceptionResNetV2 works with RGB but we can't use informations obtained from a full RGB picture\n",
    "        lab = rgb2lab(batch)\n",
    "        x = lab[:,:,:,0]#grey scale of the all batch (batch, 256,256)\n",
    "        x = x.reshape(x.shape+(1,)) #(batch, 256, 256, 1) 1 layer, 256*256, batch pics\n",
    "        y = lab[:,:,:,1:] / 128 #(batch, 256, 256, 2)\n",
    "        yield ([x, classification], y)#[x,classification] input of network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.layers import *\n",
    "from keras.models import Model\n",
    "\n",
    "encoder_input = Input(shape=(256, 256, 1,))#1 channel, 256*256 image size\n",
    "encoder = Conv2D(64, (3,3), activation='relu', padding='same', strides=2)(encoder_input)#64 filters (3x3), stride 2 \n",
    "encoder = Conv2D(128, (3,3), activation='relu', padding='same')(encoder)\n",
    "encoder = Conv2D(128, (3,3), activation='relu', padding='same', strides=2)(encoder)\n",
    "encoder = Conv2D(256, (3,3), activation='relu', padding='same')(encoder)\n",
    "encoder = Conv2D(256, (3,3), activation='relu', padding='same', strides=2)(encoder)\n",
    "encoder = Conv2D(512, (3,3), activation='relu', padding='same')(encoder)\n",
    "encoder = Conv2D(512, (3,3), activation='relu', padding='same')(encoder)\n",
    "encoder = Conv2D(256, (3,3), activation='relu', padding='same')(encoder)\n",
    "\n",
    "embed_input = Input(shape=(1000,))#num_classes: 1000 trained model (classification)\n",
    "fusion = RepeatVector(32 * 32)(embed_input) \n",
    "fusion = Reshape(([32, 32, 1000]))(fusion)\n",
    "#merge together\n",
    "fusion = concatenate([encoder, fusion], axis=3)\n",
    "fusion = Conv2D(256, (1, 1), activation='relu', padding='same')(fusion)\n",
    "\n",
    "#Decoder\n",
    "decoder = Conv2D(128, (3,3), activation='relu', padding='same')(fusion)\n",
    "decoder = UpSampling2D((2, 2))(decoder)\n",
    "decoder = Conv2D(64, (3,3), activation='relu', padding='same')(decoder)\n",
    "decoder = UpSampling2D((2, 2))(decoder)\n",
    "decoder = Conv2D(32, (3,3), activation='relu', padding='same')(decoder)\n",
    "decoder = Conv2D(16, (3,3), activation='relu', padding='same')(decoder)\n",
    "decoder = Conv2D(2, (3, 3), activation='tanh', padding='same')(decoder)\n",
    "decoder = UpSampling2D((2, 2))(decoder)\n",
    "\n",
    "model = Model(inputs=[encoder_input, embed_input], outputs=decoder)\n",
    "model.compile(optimizer='adam', loss='mse')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "Found 9294 images belonging to 1 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/massimo/anaconda/envs/tensorflow/lib/python3.6/site-packages/skimage/transform/_warps.py:110: UserWarning: Anti-aliasing will be enabled by default in skimage 0.15 to avoid aliasing artifacts when down-sampling images.\n",
      "  warn(\"Anti-aliasing will be enabled by default in skimage 0.15 to \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  18/1000 [..............................] - ETA: 3:05:08 - loss: 67.3564"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/massimo/anaconda/envs/tensorflow/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2963, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-5-741c194de6fc>\", line 5, in <module>\n",
      "    epochs=20)\n",
      "  File \"/Users/massimo/anaconda/envs/tensorflow/lib/python3.6/site-packages/keras/legacy/interfaces.py\", line 91, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/Users/massimo/anaconda/envs/tensorflow/lib/python3.6/site-packages/keras/engine/training.py\", line 2230, in fit_generator\n",
      "    class_weight=class_weight)\n",
      "  File \"/Users/massimo/anaconda/envs/tensorflow/lib/python3.6/site-packages/keras/engine/training.py\", line 1883, in train_on_batch\n",
      "    outputs = self.train_function(ins)\n",
      "  File \"/Users/massimo/anaconda/envs/tensorflow/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\", line 2482, in __call__\n",
      "    **self.session_kwargs)\n",
      "  File \"/Users/massimo/anaconda/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 905, in run\n",
      "    run_metadata_ptr)\n",
      "  File \"/Users/massimo/anaconda/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1140, in _run\n",
      "    feed_dict_tensor, options, run_metadata)\n",
      "  File \"/Users/massimo/anaconda/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1321, in _do_run\n",
      "    run_metadata)\n",
      "  File \"/Users/massimo/anaconda/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1327, in _do_call\n",
      "    return fn(*args)\n",
      "  File \"/Users/massimo/anaconda/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1312, in _run_fn\n",
      "    options, feed_dict, fetch_list, target_list, run_metadata)\n",
      "  File \"/Users/massimo/anaconda/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1420, in _call_tf_sessionrun\n",
      "    status, run_metadata)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/massimo/anaconda/envs/tensorflow/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 1863, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/massimo/anaconda/envs/tensorflow/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 1095, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/Users/massimo/anaconda/envs/tensorflow/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 311, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/Users/massimo/anaconda/envs/tensorflow/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 345, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/Users/massimo/anaconda/envs/tensorflow/lib/python3.6/inspect.py\", line 1488, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/Users/massimo/anaconda/envs/tensorflow/lib/python3.6/inspect.py\", line 1446, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/Users/massimo/anaconda/envs/tensorflow/lib/python3.6/inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/Users/massimo/anaconda/envs/tensorflow/lib/python3.6/inspect.py\", line 742, in getmodule\n",
      "    os.path.realpath(f)] = module.__name__\n",
      "  File \"/Users/massimo/anaconda/envs/tensorflow/lib/python3.6/posixpath.py\", line 388, in realpath\n",
      "    path, ok = _joinrealpath(filename[:0], filename, {})\n",
      "  File \"/Users/massimo/anaconda/envs/tensorflow/lib/python3.6/posixpath.py\", line 422, in _joinrealpath\n",
      "    if not islink(newpath):\n",
      "  File \"/Users/massimo/anaconda/envs/tensorflow/lib/python3.6/posixpath.py\", line 171, in islink\n",
      "    st = os.lstat(path)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "batch_size=10\n",
    "model.fit_generator(\n",
    "    input_generator(image_datagen, batch_size),\n",
    "    steps_per_epoch=1000,\n",
    "    epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
